# Prompt Aggregation Dataset - Custom Dataset
This dataset is a curated collection of unsafe, adversarial, and sensitive prompts.

## ğŸ§  Overview  
This dataset is a curated collection of unsafe, adversarial, and sensitive prompts designed to support research in LLM safety, prompt injection defense, and content moderation. It aggregates diverse examples from multiple benchmark sources and hackathon experiments, enabling robust evaluation and training of safety filters and classifiers.

## ğŸ“ Structure  
The dataset includes:  
- **Prompt text**: Raw user inputs or adversarial instructions.   
- **Safety labels**: Optional annotations for unsafe/safe/sensitive classification.  

## ğŸ“š Sources  
This dataset was compiled from the following public repositories and research efforts:  
- [chuyishang/safeguard](https://github.com/chuyishang/safeguard) â€“ 2024 Berkeley AI Hackathon Repo  
- [JailbreakBench/artifacts](https://github.com/JailbreakBench/artifacts) â€“ Jailbreak artifacts for JailbreakBench  
- [MurrayTom/SG-Bench](https://github.com/MurrayTom/SG-Bench) â€“ SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types  
- [facebookresearch/fastText](https://github.com/facebookresearch/fastText) â€“ Library for fast text representation and classification *(used for preprocessing and embedding tasks)*

## ğŸ“Œ Notes  
All data was collected and processed in accordance with the licenses of the original sources. Please refer to each repository for specific usage terms.

